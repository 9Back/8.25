% TEMPLATE for Usenix papers, specifically to meet requirements of
%  USENIX '05
% originally a template for producing IEEE-format articles using LaTeX.
%   written by Matthew Ward, CS Department, Worcester Polytechnic Institute.
% adapted by David Beazley for his excellent SWIG paper in Proceedings,
%   Tcl 96
% turned into a smartass generic template by De Clarke, with thanks to
%   both the above pioneers
% use at your own risk.  Complaints to /dev/null.
% make it two column with no page numbering, default is 10 point

% Munged by Fred Douglis <douglis@research.att.com> 10/97 to separate
% the .sty file from the LaTeX source template, so that people can
% more easily include the .sty file into an existing document.  Also
% changed to more closely follow the style guidelines as represented
% by the Word sample file. 

% Note that since 2010, USENIX does not require endnotes. If you want
% foot of page notes, don't include the endnotes package in the 
% usepackage command, below.

% This version uses the latex2e styles, not the very ancient 2.09 stuff.

% Updated July 2018: Text block size changed from 6.5" to 7"

\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix2019,epsfig,endnotes}

\usepackage{pgfplots}
\pgfplotsset{compat=newest}


\begin{document}

%don't want date printed
\date{}

%make title bold and 14 pt font (Latex default is non-bold, 16 pt)
\title{\Large \bf 9Back : Making Our Plans Great Again}

%for single author (just remove % characters)
\author{
{\rm Maxwell Bland}
\and
{\rm Leon Cheung}\\ \\
University of California, San Diego
\and
{\rm Kilian Pfeiffer}\\
% copy the following lines to add more authors
% \and
% {\rm Name}\\
%Name Institution
} % end author

\maketitle

% Use the following at camera-ready time to suppress page numbers.
% Comment it out when you first submit the paper for review.
\thispagestyle{empty}


\subsection*{Abstract}
The following paper describes a set of benchmarks run on the Plan 9 operating system. In particular, it uses the only actively developed branch, 9FRONT "RUN FROM ZONE!" (2018.09.09.0 Release). The goal of this project is to provide researchers and enthusiasts with greater insight into the current state of the operating system's performance and capabilities of interaction with hardware. Through tests of CPU, Memory, Network, and File System operations, we will gain insight on bottlenecks in the system's performance and the interactions between low-level (hardware) and high-level (OS) system components. These performance statistics will be contrasted with subjective experiences of ``responsiveness''.

\section{Introduction}

Plan 9 from Bell labs is a distributed operating system which emerged around the 1980's. It built upon the ideas of UNIX, but adopted an ideology that ``everything is a file''. Although the system was marketed in the 90's, it did not catch on, as prior operating systems had already gained enough of a foothold. Eventually, during the 2000's, Bell Labs ceased development on Plan 9, meaning official development halted. Unofficial\footnote{This is debatable. If you adopt an orphan, are they not your official child?} development continues on the 9front fork of the codebase, with new support for Wi-Fi, Audio, and everything anyone could ever want or need. 

The experiments were performed as a group using a shared codebase and a single machine described in the following section. The measurements were performed via programs written in Plan 9's \textit{special} version of C 99 and Alef, both described in the original Plan 9 paper todo{cite}. The compilers used were the x86 versions of Plan 9's C compiler and Alef compiler, 8c and 8al respectively. The compilers were run with no special optimization settings. Version numbers are not available. Measurements were performed on a single machine running Plan 9 directly from hardware; given the nature of the Plan 9 system, additional metrics could be established for networked file systems and CPU servers; these measurements were not done for sake of simplicity, and because results under these conditions should be inferrable from the results cataloged within this paper.

\section{Machine Description}

We ran this beautiful operating system of the gods on a Thinkpad T420, the machine of true developers.

{\tt \small
\begin{verbatim}
    Processor: model, cycle time, cache sizes (L1, L2,
      instruction, data, etc.)
      Intel(R) Core(TM) i5-2520M CPU @ 2.50GHz (Sandy Bridge)
      cache size 3072 KB
      L1$	128 KiB	
        L1I$	64 KiB	2x32 KiB	8-way set associative	 
        L1D$	64 KiB	2x32 KiB	8-way set associative	write-back
      L2$	512 KiB 2x256 KiB	8-way set associative	write-back
      L3$	3 MiB   2x1.5 MiB	12-way set associative	write-back
      cpu family 6
      model 42
      stepping 7
      siblings 4
      cores 2
      fpu yes
      fpu_exception yes
      fpu_exception yes
      bogomips 4986.98
      clflush size 64
      cache_alignment 64
    Details at https://en.wikichip.org/wiki/
    intel/core_i5/i5-520m 

    Memory bus
      DDR3-1333
      i/o-bus-frequency: 666MHz
      bus-bandwith:  10656 MB/s
      memory-clock:  166MHz
      Column Access Strobe (CAS) latency:

    I/O bus
      SataIII-speed: 600MB/s

    RAM size
      8 GB
    Disk: capacity, RPM, controller cache size
      Samsung SSD 860 EVO 500GB
      Capacity: 500GiB
      RPM: 550MB/s read, 520 MB/s write
       and 98,000 IOPS (Read QD32)
      Controller Cache Size: 512MB 
    Network card speed:
     Intel 82579 LM Gigabyte: 1Gb/s
     intel Centrino Ultimate-N 6300: 450 Mbps
      
    Operating system (including version/release) 
      9FRONT "RUN FROM ZONE!" (2018.09.09.0 Release)
\end{verbatim}
}

\section{Experiments}

For each section, we report the base hardware performance, make a guess as to how much overhead software will add to base hardware performance, combine these two into an overall prediction of performance, and then implment and perform the measurement. In all cases, we run the experiment multiple times, and calculate the standard deviation across measurements. We use the \texttt{cycles()} syscall to record the timestamp. Dynamic CPU frequency scaling was disabled for all trials, and all trials were restricted to a single core.

\subsection{Measurement Overhead (Reading Time)}

The following section reports overheads of reading time. One trial involves looping over 16 \texttt{cycles} timing calls, $2^{16}$ times. We average out for each call of \texttt{cycles}, and we do 64 trials altogether.

Since we imagine getting the current cycles clock is a fast operation, so we estimate the hardware performance to be on the scale of nanoseconds, since one clock cycle takes approximately 1 nanoseconds. We think that the \texttt{cycles} call should do something similar to reading from the cycle counter, so it may be a bit slower, perhaps on the order of 10ns.
[https://www.7-cpu.com/cpu/SandyBridge.html]

The software cost of this, however, will be tremendous, since that value must be moved out from a register and potentially to a variable, and a syscall must be made, the result might be aroudn 10 to 30 times the hardware cost.

This would put our predicted cost at around 100 to 300 nanoseconds.

\subsection{Measurement Overhead (Loop Overhead)}

In order to measure the loop overhead, we took the time at the end of a loop
iteration and at the start of a loop iteration, and find the difference between
those to measure the loop overhead time. We repreat this 16384 times within
each trial, and perform 64 trials. We average over all of these trials.

The cost of doing a single for loop based branch in hardware is also very low, so long as the branch
prediction is correct, implying that it is on the order of only a couple of nanoseconds. But 
there is the need to increment a variable, and check a loop conditional, which adds overhead. 30 to 40
cycles on a 2.4GHz clock is around 16-20 nanoseconds. So this whole check and loop might take 100 ns. 
[https://www.7-cpu.com/cpu/SandyBridge.html]

Software, again, will slow this down, as well as the rest of the operating system. Other system processes may need the processor, data may not be in the carch, and thus the latency might be 20 times this, the same as a reference to the L2 cache, roughly

Thus, our predicted cost is around 2 microseconds.

\begin{figure}
	\centering
    \begin{tabular}{lll}
         & Reading nsec & Loop \\
Hardware Guess  & 100 nsec              &  100 nsec                 \\
Software Multiplier & 10-30              & 20		\\
Prediction & 1-3 usec                     & 2 usec                  \\
Average  & 2.080 usec                     & 2.084 usec                  \\
Std Dev. & 0.006 usec                     & 0.0034 usec                
\end{tabular}
\caption{Measurement Overheads}
\label{tab:generaloverheads}
\end{figure}


\subsection{Procedure Call Overhead}

In measuring the function overhead, we created functions of zero to seven integer arguments,
we then proceeded to call eacah of these functions 20000
times, and found the average time taken over these 20000 calls. Overall, increment overhead was almost non-existent.

The hardware cost of a procedure call using the standard c calling convention should not be that high, as it is almost all done in registers. The caller pushes the arguments to the stack (very little cost), saves the return address, the frame pointer, and calls the function. The hardware cost of this should be more than a for loop, but still reasonable, maybe roughly equitable since there are no comparisons or complex operations. Thus, somewhere on the range of 150ns to 200ns would seem reasonable, maybe even 100ns.
[https://www.7-cpu.com/cpu/SandyBridge.html]

The software cost of this is is probably about the same as the measurement overhead and for loop overhead, since we still may need to context switch to another process, as well as handle interrupts, etc. So I would reason that it would be around 20x the hardware cost.

This puts our prediction for the cost to be somewhere between 2000 nsec and 4000 nsec. Arguments shouldn't really effect the speed of a function call under our system.

\begin{figure}
	\centering
\begin{tabular}{lll}
Hardware Guess       & 150 nsec & \\
Software Multiplier Guess       & 20x &  \\
Prediction       & 3000 usec &  \\
Num Args & Average    & Std Dev.   \\
0        & 2.110 usec & 0.191 usec \\
1        & 2.104 usec & 1.248 usec \\
2        & 2.118 usec & 0.346 usec \\
3        & 2.113 usec & 0.571 usec \\
4        & 2.108 usec & 0.213 usec \\
5        & 2.120 usec & 0.915 usec \\
6        & 2.111 usec & 2.165 usec \\
7        & 2.108 usec & 0.321 usec
\end{tabular}
\caption{Procedure Call Overheads}
\label{tab:proccalloverheads}
\end{figure}

\subsection{System Call Overhead}

To measure the syscall overhead, we used the \texttt{errstr(2)} syscall, in
particular, \texttt{rerrstr(char*, vlong)}. \texttt{reerrstr} reads the error
string for the error of a previous syscall, and does not clear the errstr
buffer. Within each trial, we performed the syscall 16384 times, and averaged
over 64 trials.

Data from IBM puts the cost of making a system call on the Pentium processor in 1997/1995
to be 223 cycles, or roughly 1.68 usec for that processor. Our processor is much faster,
putting the timing at around 100 nsec.
%[https://www.ibm.com/developerworks/ community/blogs/kevgrig/entry /approximate_overhead_of_system_ calls9?lang=en]

It is likely that the overhead of a syscall beyond this is pretty high due to the
actual operations and implementation of the syscall operation. Thus, the cost could be 30 to
40x the hardware cost.

That puts our estimate at around 3 to 4 usec.

\begin{figure}
	\centering
    \begin{tabular}{ll}
            & System Call Overhead \\
    Hardware Guess  & 100 nsec  \\
    Software Multiplier  & 30-40x           \\
    Prediction  & 3-4 usec           \\
    Average  & 4.835 usec           \\
    Std Dev. & 0.056 usec          
    \end{tabular}
\caption{Syscall Overhead}
\label{tab:syscalloverheads}
\end{figure}

\subsection{Task Creation Time}

We measured process creation overhead in two different ways, one in which we
\texttt{rfork}'ed a new process without copying the parent state to the child,
and, \texttt{fork}, where the child will copy all of the parent's file
descriptors, and share the other state as normal. We do these 20000 times for
each type of \texttt{fork} and we average over all of these calls. This methodology
results from the fact that Plan 9 does not have kernel threads.

The hardware costs of a light fork should be much less than a heavy fork, since
resources do not need to be shared, but because our processes are minimal, it is
likely this difference is negligible. Still, this whole process will take tens of thousands to
millions of cycles, meaning thousands of microseconds, maybe 500 usec with a 1,000,000 cycles.
[http://ithare.com/infographics-operation-costs-in-cpu-clock-cycles/]

The software costs are most likely pretty low since Plan 9's rfork mechanism is relatively light
weight and similar to the Unix version of fork. It might only scale this already tremendous
runtime by a factor of two or so.

That puts our estimate at around 1000 usec for both fork operations.

\begin{figure}
	\centering
\begin{tabular}{lll}
        & Light Fork & Heavy Fork \\
Hardware Guess & 500 usec                 & 500 usec                  \\
Software Multiplier & 2x                 & 2x                  \\
Prediction & 1000 usec                 & 1000 usec                  \\
Average & 1080 usec                 & 1114 usec                  \\
Std Dev & 278 usec                  & 344 usec                  
\end{tabular}
\caption{Fork Overheads}
\label{tab:forkoverheads}
\end{figure}

\subsection{Context Switch Time}

Finally, to measure context switching, we created a pipe in a parent process,
and forked off a new child process. We forced a context switch by writing to
the pipe in the parent and then waiting for the child to read from the pipe
and terminate. We take the time between when the parent goes to sleep and when
the parent wakes up again after the child's termination. Then, we subtract
the overhead of reading from the pipe and closing the pipe (which happens in
the child), and divide the time in half to account for the two context switches.
We repeat this measurement 1000 times and use a new pipe each time. To take
the time of a context switch, we take the minimum time over all of these trials.

We estimated the base hardware cost of context switching while pinned to a single CPU to
be on the order of around 1100 ns based upon benchmarks done on the Intel E5-2620, another
Sandy Bridge processor model. [https://blog.tsunanet.net/2010/11/how-long-does-it-take-to-make-context.html]

The software cost to a context switch for Plan 9 is going to be huge. Probably on the order of 40x or something along these lines, since the OS needs to deal with scheduling and piping and I/O and virtual memory, etc..

This puts our prediction right around 44 usec.

\subsection{RAM Access Time}
To measure ram access time allocated a $1.6$ GB array on the heap, and iterated through it with changing stride size to hit L1, L2 cache and finally memory. The stride size gets increased by factor 1.1 for each iteration.

\begin{figure}
	\centering
	% This file was created by matplotlib2tikz vx.y.z.
% This file was created by matplotlib2tikz v0.6.18.
\begin{tikzpicture}

\definecolor{color0}{rgb}{0.12156862745098,0.466666666666667,0.705882352941177}
\definecolor{color1}{rgb}{1,0.498039215686275,0.0549019607843137}

\begin{axis}[
height=6cm,
tick align=outside,
tick pos=left,
width=8cm,
x grid style={white!69.01960784313725!black},
xlabel={last index accessed in iteration},
xmin=43.6128290830085, xmax=3699138152.51333,
xmode=log,
y grid style={white!69.01960784313725!black},
ylabel={memory access latency in ns},
ymin=-202.79028656, ymax=4300.08276416
]
\addplot [semithick, color0, forget plot]
table [row sep=\\]{%
	100	24.464 \\
	200	24.848 \\
	300	24.272 \\
	400	24.56 \\
	500	24.768 \\
	600	24.768 \\
	700	24.56 \\
	800	24.864 \\
	900	25.056 \\
	1000	24.704 \\
	1200	24.608 \\
	1400	24.416 \\
	1600	24.992 \\
	1800	24.704 \\
	2000	24.928 \\
	2300	24.896 \\
	2600	24.736 \\
	2900	24.448 \\
	3200	48.112 \\
	3600	24.656 \\
	4000	24.96 \\
	4500	24.768 \\
	5000	24.672 \\
	5600	24.928 \\
	6200	24.608 \\
	6900	24.528 \\
	7600	43.104 \\
	8400	24.688 \\
	9300	24.848 \\
	10300	24.848 \\
	11400	43.328 \\
	12600	24.736 \\
	13900	24.752 \\
	15300	24.56 \\
	16900	42.72 \\
	18600	24.64 \\
	20500	42.4 \\
	22600	24.88 \\
	24900	42.336 \\
	27400	24.464 \\
	30200	42.304 \\
	33300	44.288 \\
	36700	41.952 \\
	40400	42.368 \\
	44500	42.512 \\
	49000	41.376 \\
	54000	43.68 \\
	59500	41.824 \\
	65500	58.672 \\
	72100	42.432 \\
	79400	59.952 \\
	87400	59.28 \\
	96200	59.008 \\
	105900	59.296 \\
	116500	76.16 \\
	128200	78.432 \\
	141100	75.728 \\
	155300	75.28 \\
	170900	93.072 \\
	188000	91.92 \\
	206900	113.12 \\
	227600	109.888 \\
	250400	111.104 \\
	275500	125.808 \\
	303100	145.872 \\
	333500	146.416 \\
	366900	161.792 \\
	403600	177.008 \\
	444000	175.504 \\
	488500	194.736 \\
	537400	190.128 \\
	591200	209.152 \\
	650400	249.584 \\
	715500	263.648 \\
	787100	240.736 \\
	865900	225.36 \\
	952500	374.56 \\
	1047800	341.536 \\
	1152600	336.624 \\
	1267900	416.48 \\
	1394700	377.44 \\
	1534200	409.904 \\
	1687700	448.528 \\
	1856500	400.912 \\
	2042200	449.024 \\
	2246500	471.664 \\
	2471200	546.016 \\
	2718400	532.32 \\
	2990300	593.792 \\
	3289400	651.344 \\
	3618400	650.8 \\
	3980300	678.336 \\
	4378400	691.088 \\
	4816300	712.72 \\
	5298000	726.096 \\
	5827900	796.816 \\
	6410700	826.784 \\
	7051800	820.784 \\
	7757000	853.344 \\
	8532800	868.192 \\
	9386100	889.056 \\
	10324800	931.408 \\
	11357300	997.088 \\
	12493100	969.216 \\
	13742500	942.128 \\
	15116800	1066.704 \\
	16628500	1029.728 \\
	18291400	1026.176 \\
	20120600	1083.392 \\
	22132700	1177.168 \\
	24346000	1173.344 \\
	26780700	1194.528 \\
	29458800	1221.92 \\
	32404700	1275.008 \\
	35645200	1257.776 \\
	39209800	1248.88 \\
	43130800	1247.376 \\
	47443900	1310.432 \\
	52188300	1373.488 \\
	57407200	1326.24 \\
	63148000	1401.664 \\
	69462900	1394.256 \\
	76409200	1395.184 \\
	84050200	1427.328 \\
	92455300	1409.184 \\
	101700900	1466 \\
	111871000	1457.376 \\
	123058200	1490.272 \\
	135364100	1485.952 \\
	148900600	1495.2 \\
	163790700	1520.016 \\
	180169800	1744.816 \\
	198186800	1509.936 \\
	218005500	1558.688 \\
	239806100	1558 \\
	263786800	1568.752 \\
	290165500	1572.768 \\
	319182100	1587.168 \\
	351100400	1712.096 \\
	386210500	1672.672 \\
	424831600	1656.928 \\
	467314800	2084.784 \\
	514046300	1680.656 \\
	565451000	1750.128 \\
	621996200	1917.824 \\
	684195900	1762.016 \\
	752615500	1940.72 \\
	827877100	1754.768 \\
	910664900	1919.088 \\
	1001731400	1993.776 \\
	1101904600	1817.84 \\
	1212095100	1818 \\
	1333304700	2047.264 \\
	1466635200	1849.696 \\
	1613298800	1895.664 \\
};
\addplot [semithick, color1, forget plot]
table [row sep=\\]{%
	100	3.381228 \\
	200	1.8857612 \\
	300	2.6873808 \\
	400	2.543698 \\
	500	1.9658524 \\
	600	2.054988 \\
	700	2.699926 \\
	800	1.9909556 \\
	900	1.9229312 \\
	1000	2.1144228 \\
	1200	2.1677492 \\
	1400	2.9815004 \\
	1600	2.0834432 \\
	1800	2.8290608 \\
	2000	1.9878672 \\
	2300	2.0024944 \\
	2600	2.4203932 \\
	2900	2.3740464 \\
	3200	235.2648896 \\
	3600	2.1237852 \\
	4000	2.0364676 \\
	4500	1.978832 \\
	5000	2.0261332 \\
	5600	2.1606516 \\
	6200	2.607362 \\
	6900	2.3081628 \\
	7600	184.3112248 \\
	8400	2.5625488 \\
	9300	2.1991124 \\
	10300	2.0546768 \\
	11400	181.70868 \\
	12600	2.4097932 \\
	13900	2.8572884 \\
	15300	2.7093912 \\
	16900	185.4910716 \\
	18600	2.1938096 \\
	20500	178.754804 \\
	22600	2.6520936 \\
	24900	177.1551068 \\
	27400	2.8826212 \\
	30200	177.7995736 \\
	33300	198.8315152 \\
	36700	174.301562 \\
	40400	178.5997776 \\
	44500	181.3134308 \\
	49000	169.0530076 \\
	54000	191.1726844 \\
	59500	176.7302696 \\
	65500	240.1489728 \\
	72100	177.1403752 \\
	79400	246.307978 \\
	87400	243.0030124 \\
	96200	242.7422304 \\
	105900	244.1425756 \\
	116500	293.10524 \\
	128200	308.298368 \\
	141100	293.3117324 \\
	155300	292.7411852 \\
	170900	337.8212776 \\
	188000	332.2634424 \\
	206900	389.4420048 \\
	227600	374.0075124 \\
	250400	378.2771772 \\
	275500	401.560518 \\
	303100	444.2010964 \\
	333500	446.8854992 \\
	366900	466.7522392 \\
	403600	487.1212892 \\
	444000	483.3051908 \\
	488500	513.9346096 \\
	537400	498.9257224 \\
	591200	526.9976948 \\
	650400	585.078478 \\
	715500	596.6380728 \\
	787100	561.6958744 \\
	865900	546.1151528 \\
	952500	680.7845452 \\
	1047800	656.7102272 \\
	1152600	671.5830668 \\
	1267900	719.9270896 \\
	1394700	686.8388156 \\
	1534200	708.4055856 \\
	1687700	717.8518732 \\
	1856500	691.8767892 \\
	2042200	719.431652 \\
	2246500	742.8784076 \\
	2471200	764.0413852 \\
	2718400	762.0278968 \\
	2990300	779.5049024 \\
	3289400	808.170694 \\
	3618400	804.4847852 \\
	3980300	805.81963 \\
	4378400	827.3796404 \\
	4816300	813.583288 \\
	5298000	812.2787208 \\
	5827900	826.6674804 \\
	6410700	827.8342056 \\
	7051800	817.1524568 \\
	7757000	836.5465992 \\
	8532800	838.1399528 \\
	9386100	819.7732752 \\
	10324800	829.2060368 \\
	11357300	817.6975092 \\
	12493100	967.7607936 \\
	13742500	846.9828484 \\
	15116800	807.2474496 \\
	16628500	810.7106748 \\
	18291400	835.0246412 \\
	20120600	802.2936808 \\
	22132700	789.1154536 \\
	24346000	778.2213228 \\
	26780700	781.8892132 \\
	29458800	757.3028856 \\
	32404700	739.8653724 \\
	35645200	748.2620252 \\
	39209800	762.2298696 \\
	43130800	759.0122388 \\
	47443900	740.0641868 \\
	52188300	692.26101 \\
	57407200	730.2199296 \\
	63148000	687.2342128 \\
	69462900	713.5628364 \\
	76409200	713.0307144 \\
	84050200	702.8632524 \\
	92455300	725.688978 \\
	101700900	688.8636208 \\
	111871000	681.3077692 \\
	123058200	675.5310724 \\
	135364100	668.1505776 \\
	148900600	714.51054 \\
	163790700	686.1683336 \\
	180169800	2347.8596864 \\
	198186800	694.8974916 \\
	218005500	801.7696388 \\
	239806100	698.126164 \\
	263786800	701.1556888 \\
	290165500	733.407084 \\
	319182100	739.597226 \\
	351100400	1145.294592 \\
	386210500	712.8647756 \\
	424831600	721.8743668 \\
	467314800	4095.4067164 \\
	514046300	719.498104 \\
	565451000	1074.7844208 \\
	621996200	2191.502116 \\
	684195900	758.5072876 \\
	752615500	2463.5364384 \\
	827877100	766.6261328 \\
	910664900	1701.8843168 \\
	1001731400	2281.5396896 \\
	1101904600	762.4330024 \\
	1212095100	782.6245364 \\
	1333304700	2242.7503504 \\
	1466635200	801.0178428 \\
	1613298800	945.8153744 \\
};
\end{axis}

\end{tikzpicture}
	\caption{Ram access time with size of the memory region accessed on $x$  axis in an logarithmic scale and latency in seconds on the $y$ axis}
	\label{fig:memlatency}
\end{figure}

\begin{figure}
	\centering
\begin{tabular}{ll}
Hardware Cost  & 1100 nsec  \\
Software Multiplier  & 40x   \\
Prediction  & 44 usec    \\
Average  & 42.048 usec    \\
Std Dev. & 9.569 usec     \\
Min      & 12.491 usec   
\end{tabular}
\caption{Context Switch Overheads}
\label{tab:conswitchoverheads}
\end{figure}

\subsection{Discussion}
\textit{Compare the measured performance with the predicted performance. If they are wildly different, speculate on reasons why. What may be contributing to the overhead?}
    Our hardware estimates were very, very rough. It'd be worthwhile to go back and double check these and discuss with our classmates on the numbers we arrived at.

    
    \textit{Evaluate the success of your methodology. How accurate do you think your results are?}
    Our results agree with the Plan 9 paper, which is suprising given that our processor is a bit faster. It will be worth it to discuss with Geoff our results and what we might try looking into going forward. It was hard to ensure we were taking the best approach. We certainly measured roughly what we wanted to measure.

\textit{Answer any questions specifically mentioned with the operation.}
The issue here is that there are no kernel threads in Plan 9, so some of the CPU measurement questions did not apply

\section{Conclusion}

We should try to correct our idea of hardware cycle counts for typical instructions and discuss the manners in which we chose to benchmark in order to make sure they are the absolute best way to measure these values, or something close to it. 

{\normalsize \bibliographystyle{acm}
\bibliography{../common/bibliography}}


%\theendnotes

\end{document}







